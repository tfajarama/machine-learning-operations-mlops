{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-14T11:00:08.252425Z",
     "start_time": "2025-01-14T10:59:59.600265Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Text\n",
    "\n",
    "from absl import logging\n",
    "from tfx.orchestration import metadata, pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "PIPELINE_NAME = \"tfajarama-pipeline\"\n",
    " \n",
    "# pipeline inputs\n",
    "DATA_ROOT = \"data\"\n",
    "TRANSFORM_MODULE_FILE = \"modules/loan_approval_transform.py\"\n",
    "TRAINER_MODULE_FILE = \"modules/loan_approval_trainer.py\"\n",
    "TUNER_MODULE_FILE = \"modules/loan_approval_tuner.py\"\n",
    " \n",
    "# pipeline outputs\n",
    "OUTPUT_BASE = \"./\"\n",
    "serving_model_dir = os.path.join(OUTPUT_BASE, 'serving_model')\n",
    "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
    "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T11:00:08.282307Z",
     "start_time": "2025-01-14T11:00:08.259421Z"
    }
   },
   "id": "eefb2742823bcc68",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def init_local_pipeline(\n",
    "    components, pipeline_root: Text\n",
    ") -> pipeline.Pipeline:\n",
    "    \n",
    "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
    "    beam_args = [\n",
    "        \"--direct_running_mode=multi_processing\",\n",
    "        # 0 auto-detect based on the number of CPUs available \n",
    "        # during execution time.\n",
    "        \"--direct_num_workers=0\" \n",
    "    ]\n",
    "    \n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=PIPELINE_NAME,\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=components,\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
    "            metadata_path\n",
    "        ),\n",
    "        beam_pipeline_args=beam_args\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T11:00:08.313943Z",
     "start_time": "2025-01-14T11:00:08.288959Z"
    }
   },
   "id": "aee1299442e4ba18",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Starting the pipeline...\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Pipeline root set to: ./tfajarama-pipeline\n",
      "INFO:absl:Generating ephemeral wheel package for 'D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\submission_2\\\\modules\\\\loan_approval_transform.py' (including modules: ['components', 'loan_approval_trainer', 'loan_approval_transform', 'loan_approval_tuner']).\n",
      "INFO:absl:User module package has hash fingerprint version 966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359.\n",
      "INFO:absl:Executing: ['D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\Scripts\\\\python.exe', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmp5uap_wxt\\\\_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmpglhptp5l', '--dist-dir', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmpz8d0dpc6']\n",
      "INFO:absl:Successfully built user code wheel distribution at './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'; target user module is 'loan_approval_transform'.\n",
      "INFO:absl:Full user module path is 'loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'\n",
      "INFO:absl:Generating ephemeral wheel package for 'D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\submission_2\\\\modules\\\\loan_approval_trainer.py' (including modules: ['components', 'loan_approval_trainer', 'loan_approval_transform', 'loan_approval_tuner']).\n",
      "INFO:absl:User module package has hash fingerprint version 966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359.\n",
      "INFO:absl:Executing: ['D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\Scripts\\\\python.exe', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmpxcu7nmk5\\\\_tfx_generated_setup.py', 'bdist_wheel', '--bdist-dir', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmp2a35bheq', '--dist-dir', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmphko_20dw']\n",
      "INFO:absl:Successfully built user code wheel distribution at './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'; target user module is 'loan_approval_trainer'.\n",
      "INFO:absl:Full user module path is 'loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'\n",
      "INFO:absl:Using deployment config:\n",
      " executor_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.example_gen.csv_example_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=0\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=0\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Evaluator\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.evaluator.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=0\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=0\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"ExampleValidator\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_validator.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Pusher\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.pusher.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"SchemaGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.schema_gen.executor.Executor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"StatisticsGen\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.statistics_gen.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=0\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=0\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Trainer\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.trainer.executor.GenericExecutor\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "executor_specs {\n",
      "  key: \"Transform\"\n",
      "  value {\n",
      "    beam_executable_spec {\n",
      "      python_executor_spec {\n",
      "        class_path: \"tfx.components.transform.executor.Executor\"\n",
      "      }\n",
      "      beam_pipeline_args: \"--direct_running_mode=multi_processing\"\n",
      "      beam_pipeline_args: \"--direct_num_workers=0\"\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_running_mode=multi_processing\"\n",
      "        }\n",
      "      }\n",
      "      beam_pipeline_args_placeholders {\n",
      "        value {\n",
      "          string_value: \"--direct_num_workers=0\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "custom_driver_specs {\n",
      "  key: \"CsvExampleGen\"\n",
      "  value {\n",
      "    python_class_executable_spec {\n",
      "      class_path: \"tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "metadata_connection_config {\n",
      "  database_connection_config {\n",
      "    sqlite {\n",
      "      filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "      connection_mode: READWRITE_OPENCREATE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using connection config:\n",
      " sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Node CsvExampleGen depends on [].\n",
      "INFO:absl:Node CsvExampleGen is scheduled.\n",
      "INFO:absl:Node Latest_blessed_model_resolver depends on [].\n",
      "INFO:absl:Node Latest_blessed_model_resolver is scheduled.\n",
      "INFO:absl:Node StatisticsGen depends on ['Run[CsvExampleGen]'].\n",
      "INFO:absl:Node StatisticsGen is scheduled.\n",
      "INFO:absl:Node SchemaGen depends on ['Run[StatisticsGen]'].\n",
      "INFO:absl:Node SchemaGen is scheduled.\n",
      "INFO:absl:Node ExampleValidator depends on ['Run[SchemaGen]', 'Run[StatisticsGen]'].\n",
      "INFO:absl:Node ExampleValidator is scheduled.\n",
      "INFO:absl:Node Transform depends on ['Run[CsvExampleGen]', 'Run[SchemaGen]'].\n",
      "INFO:absl:Node Transform is scheduled.\n",
      "INFO:absl:Node Trainer depends on ['Run[SchemaGen]', 'Run[Transform]'].\n",
      "INFO:absl:Node Trainer is scheduled.\n",
      "INFO:absl:Node Evaluator depends on ['Run[CsvExampleGen]', 'Run[Latest_blessed_model_resolver]', 'Run[Trainer]'].\n",
      "INFO:absl:Node Evaluator is scheduled.\n",
      "INFO:absl:Node Pusher depends on ['Run[Evaluator]', 'Run[Trainer]'].\n",
      "INFO:absl:Node Pusher is scheduled.\n",
      "INFO:absl:node CsvExampleGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 8,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type pipeline and name tfajarama-pipeline\n",
      "DEBUG:absl:Registering a metadata type with id 10.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"pipeline\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline\"\n",
      "  }\n",
      "}\n",
      " is 1.\n",
      "DEBUG:absl:Failed to get context of type pipeline_run and name 20250114-180011.700094\n",
      "DEBUG:absl:Registering a metadata type with id 11.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"pipeline_run\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"20250114-180011.700094\"\n",
      "  }\n",
      "}\n",
      " is 2.\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.CsvExampleGen\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "  }\n",
      "}\n",
      " is 3.\n",
      "INFO:absl:[CsvExampleGen] Resolved inputs: ({},)\n",
      "DEBUG:absl:Registering a metadata type with id 13.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 13\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"input_base\"\n",
      "  value {\n",
      "    string_value: \"data\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_config\"\n",
      "  value {\n",
      "    string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"output_config\"\n",
      "  value {\n",
      "    string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 8,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"output_data_format\"\n",
      "  value {\n",
      "    int_value: 6\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"output_file_format\"\n",
      "  value {\n",
      "    int_value: 5\n",
      "  }\n",
      "}\n",
      "name: \"16d2ea13-693f-4766-a422-0d08e5bef57a\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\CsvExampleGen\\examples\\1\n",
      "DEBUG:absl:Processing input data.\n",
      "INFO:absl:select span and version = (0, None)\n",
      "INFO:absl:latest span and version = (0, None)\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name a0ef7b784c30ef54aa4b88938dfe2d81dd3ce4cd8c0b5c00d73d8ce6171f7e6e\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"a0ef7b784c30ef54aa4b88938dfe2d81dd3ce4cd8c0b5c00d73d8ce6171f7e6e\"\n",
      "  }\n",
      "}\n",
      " is 4.\n",
      "INFO:absl:Going to run a new execution 1\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=1, input_dict={}, output_dict=defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}), exec_properties={'input_base': 'data', 'input_config': '{\\n  \"splits\": [\\n    {\\n      \"name\": \"single_split\",\\n      \"pattern\": \"*\"\\n    }\\n  ]\\n}', 'output_data_format': 6, 'output_file_format': 5, 'output_config': '{\\n  \"split_config\": {\\n    \"splits\": [\\n      {\\n        \"hash_buckets\": 8,\\n        \"name\": \"train\"\\n      },\\n      {\\n        \"hash_buckets\": 2,\\n        \"name\": \"eval\"\\n      }\\n    ]\\n  }\\n}', 'span': 0, 'version': None, 'input_fingerprint': 'split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350'}, execution_output_uri='./tfajarama-pipeline\\\\CsvExampleGen\\\\.system\\\\executor_execution\\\\1\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\CsvExampleGen\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\CsvExampleGen\\\\.system\\\\executor_execution\\\\1\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_gen.csv_example_gen.component.CsvExampleGen\"\n",
      "  }\n",
      "  id: \"CsvExampleGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"input_base\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"data\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"input_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 8,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_data_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 6\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"output_file_format\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 5\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tfx to temp dir C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpi21rxs5p\\build\\tfx\n",
      "INFO:absl:Generating a temp setup file at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpi21rxs5p\\build\\tfx\\setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpi21rxs5p\\build\\tfx\\setup.log\n",
      "INFO:absl:Added --extra_package=C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpi21rxs5p\\build\\tfx\\dist\\tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "DEBUG:absl:Starting Executor execution.\n",
      "DEBUG:absl:Inputs for Executor are: {}\n",
      "DEBUG:absl:Outputs for Executor are: {\"examples\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\", \"custom_properties\": {\"span\": {\"int_value\": \"0\"}, \"input_fingerprint\": {\"string_value\": \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"}}}, \"artifact_type\": {\"name\": \"Examples\", \"properties\": {\"version\": \"INT\", \"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"DATASET\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Examples\"}]}\n",
      "DEBUG:absl:Execution properties for Executor are: {\"input_base\": \"data\", \"input_config\": \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"single_split\\\",\\n      \\\"pattern\\\": \\\"*\\\"\\n    }\\n  ]\\n}\", \"output_data_format\": 6, \"output_file_format\": 5, \"output_config\": \"{\\n  \\\"split_config\\\": {\\n    \\\"splits\\\": [\\n      {\\n        \\\"hash_buckets\\\": 8,\\n        \\\"name\\\": \\\"train\\\"\\n      },\\n      {\\n        \\\"hash_buckets\\\": 2,\\n        \\\"name\\\": \\\"eval\\\"\\n      }\\n    ]\\n  }\\n}\", \"span\": 0, \"input_fingerprint\": \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"}\n",
      "INFO:absl:Generating examples.\n",
      "INFO:absl:Processing input csv data data\\* to TFExample.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852447   nanos: 836276531 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852447   nanos: 851280689 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852447   nanos: 875280857 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852447   nanos: 889284849 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852447   nanos: 980826854 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852447   nanos: 993825674 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 175958156 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 189960241 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 233487844 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 244488716 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 244488716 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 254486322 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 256486415 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 267485857 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 272486925 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852448   nanos: 286487102 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 511459589 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_51\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 512458324 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_50\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 519458293 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_55\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 537469863 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_54\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 539471387 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_52\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 536469936 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_53\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 537469863 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_57\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852469   nanos: 537469863 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_56\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "INFO:absl:Examples generated.\n",
      "INFO:absl:Value type <class 'NoneType'> of key version in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Value type <class 'list'> of key _beam_pipeline_args in exec_properties is not supported, going to drop it\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 1 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'examples': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}) for execution 1\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 15.\n",
      "INFO:absl:node CsvExampleGen is finished.\n",
      "INFO:absl:node Latest_blessed_model_resolver is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "  }\n",
      "  id: \"Latest_blessed_model_resolver\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Latest_blessed_model_resolver\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"_generated_model_3\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"_generated_modelblessing_4\"\n",
      "    value {\n",
      "      channels {\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      hidden: true\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      input_graph_ref {\n",
      "        graph_id: \"graph_1\"\n",
      "        key: \"model_blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  input_graphs {\n",
      "    key: \"graph_1\"\n",
      "    value {\n",
      "      nodes {\n",
      "        key: \"dict_2\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          dict_node {\n",
      "            node_ids {\n",
      "              key: \"model\"\n",
      "              value: \"input_3\"\n",
      "            }\n",
      "            node_ids {\n",
      "              key: \"model_blessing\"\n",
      "              value: \"input_4\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_3\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_model_3\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"input_4\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_LIST\n",
      "          input_node {\n",
      "            input_key: \"_generated_modelblessing_4\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      nodes {\n",
      "        key: \"op_1\"\n",
      "        value {\n",
      "          output_data_type: ARTIFACT_MULTIMAP\n",
      "          op_node {\n",
      "            op_type: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "            args {\n",
      "              node_id: \"dict_2\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      result_node: \"op_1\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "downstream_nodes: \"Evaluator\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Running as an resolver node.\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.Latest_blessed_model_resolver\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.Latest_blessed_model_resolver\"\n",
      "  }\n",
      "}\n",
      " is 5.\n",
      "INFO:absl:[Latest_blessed_model_resolver] Resolved inputs: ({'model_blessing': [], 'model': []},)\n",
      "DEBUG:absl:Registering a metadata type with id 16.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 16\n",
      "last_known_state: RUNNING\n",
      "name: \"4f181104-a71e-4bb5-8f89-2c964d3c900f\"\n",
      "\n",
      "INFO:absl:node Latest_blessed_model_resolver is finished.\n",
      "INFO:absl:node StatisticsGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.StatisticsGen\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "  }\n",
      "}\n",
      " is 6.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[StatisticsGen] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852477657\n",
      "last_update_time_since_epoch: 1736852477657\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 17.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 17\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"exclude_splits\"\n",
      "  value {\n",
      "    string_value: \"[]\"\n",
      "  }\n",
      "}\n",
      "name: \"9ddbaeae-b654-48b7-90ee-16a83ac30739\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\StatisticsGen\\statistics\\3\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name bd2552a758ea00afe61483ce589598fd4cd8b2c65e2e8e9d8e148f3adb63a83d\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"bd2552a758ea00afe61483ce589598fd4cd8b2c65e2e8e9d8e148f3adb63a83d\"\n",
      "  }\n",
      "}\n",
      " is 7.\n",
      "INFO:absl:Going to run a new execution 3\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=3, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852477657\n",
      "last_update_time_since_epoch: 1736852477657\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='./tfajarama-pipeline\\\\StatisticsGen\\\\.system\\\\executor_execution\\\\3\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\StatisticsGen\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\StatisticsGen\\\\.system\\\\executor_execution\\\\3\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"StatisticsGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"SchemaGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tfx to temp dir C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdr40o49j\\build\\tfx\n",
      "INFO:absl:Generating a temp setup file at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdr40o49j\\build\\tfx\\setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdr40o49j\\build\\tfx\\setup.log\n",
      "INFO:absl:Added --extra_package=C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdr40o49j\\build\\tfx\\dist\\tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "DEBUG:absl:Starting Executor execution.\n",
      "DEBUG:absl:Inputs for Executor are: {\"examples\": [{\"artifact\": {\"id\": \"1\", \"type_id\": \"15\", \"uri\": \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\", \"properties\": {\"split_names\": {\"string_value\": \"[\\\"train\\\", \\\"eval\\\"]\"}}, \"custom_properties\": {\"input_fingerprint\": {\"string_value\": \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"}, \"payload_format\": {\"string_value\": \"FORMAT_TF_EXAMPLE\"}, \"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}, \"file_format\": {\"string_value\": \"tfrecords_gzip\"}, \"span\": {\"int_value\": \"0\"}, \"state\": {\"string_value\": \"published\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852477657\", \"last_update_time_since_epoch\": \"1736852477657\"}, \"artifact_type\": {\"id\": \"15\", \"name\": \"Examples\", \"properties\": {\"version\": \"INT\", \"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"DATASET\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Examples\"}]}\n",
      "DEBUG:absl:Outputs for Executor are: {\"statistics\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"}, \"artifact_type\": {\"name\": \"ExampleStatistics\", \"properties\": {\"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"STATISTICS\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ExampleStatistics\"}]}\n",
      "DEBUG:absl:Execution properties for Executor are: {\"exclude_splits\": \"[]\"}\n",
      "INFO:absl:Generating statistics for split train.\n",
      "INFO:absl:Statistics for split train written to ./tfajarama-pipeline\\StatisticsGen\\statistics\\3\\Split-train.\n",
      "INFO:absl:Generating statistics for split eval.\n",
      "INFO:absl:Statistics for split eval written to ./tfajarama-pipeline\\StatisticsGen\\statistics\\3\\Split-eval.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 142820119 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 154819488 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 376938819 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 387467861 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 487169981 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 499171257 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 534168004 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 545171022 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 579180240 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 591711759 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 604709863 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 616711854 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 617714643 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 631712436 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 659709930 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852513   nanos: 671725988 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 124282836 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_163\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 134282827 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_164\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 142288208 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_168\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 142288208 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_169\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 144286870 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_166\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 157160758 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_165\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 158720731 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_167\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852523   nanos: 168083906 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_170\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 3 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'statistics': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 3\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 18.\n",
      "INFO:absl:node StatisticsGen is finished.\n",
      "INFO:absl:node SchemaGen is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"SchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"infer_feature_shape\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.SchemaGen\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "  }\n",
      "}\n",
      " is 8.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[SchemaGen] Resolved inputs: ({'statistics': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852532943\n",
      "last_update_time_since_epoch: 1736852532943\n",
      ", artifact_type: id: 18\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 19.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 19\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"exclude_splits\"\n",
      "  value {\n",
      "    string_value: \"[]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"infer_feature_shape\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "name: \"e5dde73a-f787-4ec3-b06e-20ca5070f08b\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\SchemaGen\\schema\\4\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name 2ef9060a6bfcf9c4de6d536650e8b1a2dbad6a007f87325589f32dd6af14e426\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"2ef9060a6bfcf9c4de6d536650e8b1a2dbad6a007f87325589f32dd6af14e426\"\n",
      "  }\n",
      "}\n",
      " is 9.\n",
      "INFO:absl:Going to run a new execution 4\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=4, input_dict={'statistics': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852532943\n",
      "last_update_time_since_epoch: 1736852532943\n",
      ", artifact_type: id: 18\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}), exec_properties={'exclude_splits': '[]', 'infer_feature_shape': 1}, execution_output_uri='./tfajarama-pipeline\\\\SchemaGen\\\\.system\\\\executor_execution\\\\4\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\SchemaGen\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\SchemaGen\\\\.system\\\\executor_execution\\\\4\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "    base_type: PROCESS\n",
      "  }\n",
      "  id: \"SchemaGen\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"infer_feature_shape\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 1\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "downstream_nodes: \"ExampleValidator\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Transform\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "INFO:absl:Processing schema from statistics for split train.\n",
      "INFO:absl:Processing schema from statistics for split eval.\n",
      "INFO:absl:Schema written to ./tfajarama-pipeline\\SchemaGen\\schema\\4\\schema.pbtxt.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 4 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'schema': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")]}) for execution 4\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 20.\n",
      "INFO:absl:node SchemaGen is finished.\n",
      "INFO:absl:node Transform is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"Transform\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Transform\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.Transform\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.Transform\"\n",
      "  }\n",
      "}\n",
      " is 10.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Transform] Resolved inputs: ({'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852477657\n",
      "last_update_time_since_epoch: 1736852477657\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852533882\n",
      "last_update_time_since_epoch: 1736852533882\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 21.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 21\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"custom_config\"\n",
      "  value {\n",
      "    string_value: \"null\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"disable_statistics\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"force_tf_compat_v1\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"module_path\"\n",
      "  value {\n",
      "    string_value: \"loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\"\n",
      "  }\n",
      "}\n",
      "name: \"2d60f386-bbaa-4e1f-a9b6-522c78d86a9a\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\transform_graph\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\pre_transform_schema\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\post_transform_anomalies\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\pre_transform_stats\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\post_transform_schema\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\transformed_examples\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\updated_analyzer_cache\\5\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Transform\\post_transform_stats\\5\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name 297c7a232081eed69ddd3b7c2298e7895439eb297397df75c4d79adf371e9123\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"297c7a232081eed69ddd3b7c2298e7895439eb297397df75c4d79adf371e9123\"\n",
      "  }\n",
      "}\n",
      " is 11.\n",
      "INFO:absl:Going to run a new execution 5\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=5, input_dict={'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852477657\n",
      "last_update_time_since_epoch: 1736852477657\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852533882\n",
      "last_update_time_since_epoch: 1736852533882\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'transform_graph': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\pre_transform_schema\\\\5\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\post_transform_anomalies\\\\5\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\pre_transform_stats\\\\5\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\post_transform_schema\\\\5\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\updated_analyzer_cache\\\\5\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\post_transform_stats\\\\5\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}), exec_properties={'module_path': 'loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl', 'custom_config': 'null', 'disable_statistics': 0, 'force_tf_compat_v1': 0}, execution_output_uri='./tfajarama-pipeline\\\\Transform\\\\.system\\\\executor_execution\\\\5\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\Transform\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\Transform\\\\.system\\\\executor_execution\\\\5\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.transform.component.Transform\"\n",
      "    base_type: TRANSFORM\n",
      "  }\n",
      "  id: \"Transform\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Transform\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"post_transform_anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"post_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_schema\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Schema\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"pre_transform_stats\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleStatistics\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          base_type: STATISTICS\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformGraph\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"transformed_examples\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Examples\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "          properties {\n",
      "            key: \"version\"\n",
      "            value: INT\n",
      "          }\n",
      "          base_type: DATASET\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"updated_analyzer_cache\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"TransformCache\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"disable_statistics\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"force_tf_compat_v1\"\n",
      "    value {\n",
      "      field_value {\n",
      "        int_value: 0\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "downstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tfx to temp dir C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp1_265nm6\\build\\tfx\n",
      "INFO:absl:Generating a temp setup file at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp1_265nm6\\build\\tfx\\setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp1_265nm6\\build\\tfx\\setup.log\n",
      "INFO:absl:Added --extra_package=C:\\Users\\Asus\\AppData\\Local\\Temp\\tmp1_265nm6\\build\\tfx\\dist\\tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "DEBUG:absl:Starting Executor execution.\n",
      "DEBUG:absl:Inputs for Executor are: {\"examples\": [{\"artifact\": {\"id\": \"1\", \"type_id\": \"15\", \"uri\": \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\", \"properties\": {\"split_names\": {\"string_value\": \"[\\\"train\\\", \\\"eval\\\"]\"}}, \"custom_properties\": {\"file_format\": {\"string_value\": \"tfrecords_gzip\"}, \"input_fingerprint\": {\"string_value\": \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"}, \"payload_format\": {\"string_value\": \"FORMAT_TF_EXAMPLE\"}, \"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}, \"span\": {\"int_value\": \"0\"}, \"state\": {\"string_value\": \"published\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852477657\", \"last_update_time_since_epoch\": \"1736852477657\"}, \"artifact_type\": {\"id\": \"15\", \"name\": \"Examples\", \"properties\": {\"version\": \"INT\", \"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"DATASET\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Examples\"}], \"schema\": [{\"artifact\": {\"id\": \"3\", \"type_id\": \"20\", \"uri\": \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\", \"custom_properties\": {\"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}, \"state\": {\"string_value\": \"published\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852533882\", \"last_update_time_since_epoch\": \"1736852533882\"}, \"artifact_type\": {\"id\": \"20\", \"name\": \"Schema\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Schema\"}]}\n",
      "DEBUG:absl:Outputs for Executor are: {\"transform_graph\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\"}, \"artifact_type\": {\"name\": \"TransformGraph\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"TransformGraph\"}], \"pre_transform_schema\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\pre_transform_schema\\\\5\"}, \"artifact_type\": {\"name\": \"Schema\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Schema\"}], \"post_transform_anomalies\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\post_transform_anomalies\\\\5\"}, \"artifact_type\": {\"name\": \"ExampleAnomalies\", \"properties\": {\"split_names\": \"STRING\", \"span\": \"INT\"}}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ExampleAnomalies\"}], \"pre_transform_stats\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\pre_transform_stats\\\\5\"}, \"artifact_type\": {\"name\": \"ExampleStatistics\", \"properties\": {\"span\": \"INT\", \"split_names\": \"STRING\"}, \"base_type\": \"STATISTICS\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ExampleStatistics\"}], \"post_transform_schema\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\post_transform_schema\\\\5\"}, \"artifact_type\": {\"name\": \"Schema\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Schema\"}], \"transformed_examples\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\"}, \"artifact_type\": {\"name\": \"Examples\", \"properties\": {\"span\": \"INT\", \"version\": \"INT\", \"split_names\": \"STRING\"}, \"base_type\": \"DATASET\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Examples\"}], \"updated_analyzer_cache\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\updated_analyzer_cache\\\\5\"}, \"artifact_type\": {\"name\": \"TransformCache\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"TransformCache\"}], \"post_transform_stats\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Transform\\\\post_transform_stats\\\\5\"}, \"artifact_type\": {\"name\": \"ExampleStatistics\", \"properties\": {\"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"STATISTICS\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ExampleStatistics\"}]}\n",
      "DEBUG:absl:Execution properties for Executor are: {\"module_path\": \"loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\", \"custom_config\": \"null\", \"disable_statistics\": 0, \"force_tf_compat_v1\": 0}\n",
      "INFO:absl:Analyze the 'train' split and transform all splits when splits_config is not set.\n",
      "DEBUG:absl:Using temp path ./tfajarama-pipeline\\Transform\\transform_graph\\5\\.temp_path for tft.beam\n",
      "INFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': 'loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl', 'preprocessing_fn': None} 'preprocessing_fn'\n",
      "INFO:absl:Installing './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmpjvuksn5o', './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'.\n",
      "INFO:absl:udf_utils.get_fn {'module_file': None, 'module_path': 'loan_approval_transform@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl', 'stats_options_updater_fn': None} 'stats_options_updater_fn'\n",
      "INFO:absl:Installing './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmp43ph14bw', './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'.\n",
      "INFO:absl:Installing './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmpn0prxgxo', './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Transform-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'.\n",
      "DEBUG:absl:Inputs to executor.Transform function: {'disable_statistics': False, 'schema_path': './tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\\\\schema.pbtxt', 'examples_data_format': 6, 'data_view_uri': None, 'analyze_data_paths': ['./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\\\\Split-train\\\\*'], 'analyze_paths_file_formats': ['tfrecords_gzip'], 'transform_data_paths': ['./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\\\\Split-train\\\\*', './tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\\\\Split-eval\\\\*'], 'transform_paths_file_formats': ['tfrecords_gzip', 'tfrecords_gzip'], 'preprocessing_fn': <function preprocessing_fn at 0x000001CFB18AD1F0>, 'stats_options_updater_fn': None, 'make_beam_pipeline_fn': <bound method BaseBeamExecutor._make_beam_pipeline of <tfx.components.transform.executor.Executor object at 0x000001CFB1E2AAF0>>, 'force_tf_compat_v1': False}\n",
      "DEBUG:absl:Outputs to executor.Transform function: {'transform_output_path': './tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5', 'transform_materialize_output_paths': ['./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\\\\Split-train\\\\transformed_examples', './tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\\\\Split-eval\\\\transformed_examples'], 'temp_path': './tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\\\\.temp_path', 'pre_transform_output_stats_path': './tfajarama-pipeline\\\\Transform\\\\pre_transform_stats\\\\5', 'pre_transform_output_schema_path': './tfajarama-pipeline\\\\Transform\\\\pre_transform_schema\\\\5', 'post_transform_output_anomalies_path': './tfajarama-pipeline\\\\Transform\\\\post_transform_anomalies\\\\5', 'post_transform_output_stats_path': './tfajarama-pipeline\\\\Transform\\\\post_transform_stats\\\\5', 'post_transform_output_schema_path': './tfajarama-pipeline\\\\Transform\\\\post_transform_schema\\\\5', 'cache_output_path': './tfajarama-pipeline\\\\Transform\\\\updated_analyzer_cache\\\\5'}\n",
      "DEBUG:absl:Force tf.compat.v1: False\n",
      "DEBUG:absl:Analyze data patterns: [(0, './tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\\\\Split-train\\\\*')]\n",
      "DEBUG:absl:Transform data patterns: [(0, './tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\\\\Split-train\\\\*'), (1, './tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\\\\Split-eval\\\\*')]\n",
      "DEBUG:absl:Transform materialization output paths: [(0, './tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\\\\Split-train\\\\transformed_examples'), (1, './tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\\\\Split-eval\\\\transformed_examples')]\n",
      "DEBUG:absl:Transform output path: ./tfajarama-pipeline\\Transform\\transform_graph\\5\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tensorflow_transform\\tf_utils.py:325: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tensorflow_transform\\tf_utils.py:325: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[<class 'str'>, Union[<class 'NoneType'>, <class 'tfx.components.transform.executor._Dataset'>]], Union[<class 'NoneType'>, Dict[<class 'str'>, Dict[<class 'str'>, <class 'apache_beam.pvalue.PCollection'>]]], <class 'int'>] instead.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_2/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_3/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_4/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[<class 'str'>, Union[<class 'NoneType'>, <class 'tfx.components.transform.executor._Dataset'>]], Union[<class 'NoneType'>, Dict[<class 'str'>, Dict[<class 'str'>, <class 'apache_beam.pvalue.PCollection'>]]], <class 'int'>] instead.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "INFO:absl:If the number of unique tokens is smaller than the provided top_k or approximation error is acceptable, consider using tft.experimental.approximate_vocabulary for a potentially more efficient implementation.\n",
      "DEBUG:absl:Using existing cache in: None\n",
      "WARNING:root:This input type hint will be ignored and not used for type-checking purposes. Typically, input type hints for a PTransform are single (or nested) types wrapped by a PCollection, or PBegin. Got: Dict[<class 'tensorflow_transform.beam.analyzer_cache.DatasetKey'>, <class 'tensorflow_transform.beam.analyzer_cache.DatasetCache'>] instead.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: List[<class 'apache_beam.pvalue.PDone'>] instead.\n",
      "WARNING:root:This input type hint will be ignored and not used for type-checking purposes. Typically, input type hints for a PTransform are single (or nested) types wrapped by a PCollection, or PBegin. Got: Dict[<class 'tensorflow_transform.beam.analyzer_cache.DatasetKey'>, <class 'tensorflow_transform.beam.analyzer_cache.DatasetCache'>] instead.\n",
      "WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: List[<class 'apache_beam.pvalue.PDone'>] instead.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature previous_loan_defaults_on_file has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_gender has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_intent has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_education has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature cb_person_cred_hist_length has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature credit_score has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_amnt has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_int_rate has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_percent_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature loan_status has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_age has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_emp_exp has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_home_ownership has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "INFO:absl:Feature person_income has a shape dim {\n",
      "  size: 1\n",
      "}\n",
      ". Setting to DenseTensor.\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852587   nanos: 848777770 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852587   nanos: 865779161 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852587   nanos: 875778198 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852587   nanos: 898777961 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 211399793 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 223391294 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 503016233 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 504015922 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 515014410 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 516013860 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 580555200 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 581560373 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 581560373 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 599555730 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 602554798 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852588   nanos: 603566169 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852598   nanos: 747849702 } message: \"From D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow_transform\\\\tf_utils.py:325: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\\nInstructions for updating:\\nUse ref() instead.\" instruction_id: \"bundle_533\" transform_id: \"Analyze/CreateSavedModelForAnalyzerInputs[Phase0][tf_v2_only]/CreateSavedModel\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\util\\\\deprecation.py:350\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 106262445 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_923\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 147263288 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_921\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 166798353 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_924\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 179799079 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_922\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 195795536 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_919\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 199692010 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_920\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 203691720 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_918\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852615   nanos: 247992277 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_917\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-14\" \n",
      "DEBUG:absl:Cleaning up temp path ./tfajarama-pipeline\\Transform\\transform_graph\\5\\.temp_path on executor success\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 5 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'transform_graph': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\"\n",
      ", artifact_type: name: \"TransformGraph\"\n",
      ")], 'pre_transform_schema': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\pre_transform_schema\\\\5\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'post_transform_anomalies': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\post_transform_anomalies\\\\5\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")], 'pre_transform_stats': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\pre_transform_stats\\\\5\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'post_transform_schema': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\post_transform_schema\\\\5\"\n",
      ", artifact_type: name: \"Schema\"\n",
      ")], 'transformed_examples': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\"\n",
      ", artifact_type: name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'updated_analyzer_cache': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\updated_analyzer_cache\\\\5\"\n",
      ", artifact_type: name: \"TransformCache\"\n",
      ")], 'post_transform_stats': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Transform\\\\post_transform_stats\\\\5\"\n",
      ", artifact_type: name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")]}) for execution 5\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 22.\n",
      "DEBUG:absl:Registering a metadata type with id 20.\n",
      "DEBUG:absl:Registering a metadata type with id 23.\n",
      "DEBUG:absl:Registering a metadata type with id 18.\n",
      "DEBUG:absl:Registering a metadata type with id 20.\n",
      "DEBUG:absl:Registering a metadata type with id 15.\n",
      "DEBUG:absl:Registering a metadata type with id 24.\n",
      "DEBUG:absl:Registering a metadata type with id 18.\n",
      "INFO:absl:node Transform is finished.\n",
      "INFO:absl:node ExampleValidator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.ExampleValidator\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.ExampleValidator\"\n",
      "  }\n",
      "}\n",
      " is 12.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[ExampleValidator] Resolved inputs: ({'statistics': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852532943\n",
      "last_update_time_since_epoch: 1736852532943\n",
      ", artifact_type: id: 18\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852533882\n",
      "last_update_time_since_epoch: 1736852533882\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 25.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 25\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"exclude_splits\"\n",
      "  value {\n",
      "    string_value: \"[]\"\n",
      "  }\n",
      "}\n",
      "name: \"accf2f7f-d47f-4463-a64a-cbe9bb2d6240\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\ExampleValidator\\anomalies\\6\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name 15fb8ae127b651684809cde03905317aa438f4c3da8e331d405e5bb912b7c760\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"15fb8ae127b651684809cde03905317aa438f4c3da8e331d405e5bb912b7c760\"\n",
      "  }\n",
      "}\n",
      " is 13.\n",
      "INFO:absl:Going to run a new execution 6\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=6, input_dict={'statistics': [Artifact(artifact: id: 2\n",
      "type_id: 18\n",
      "uri: \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852532943\n",
      "last_update_time_since_epoch: 1736852532943\n",
      ", artifact_type: id: 18\n",
      "name: \"ExampleStatistics\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "base_type: STATISTICS\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852533882\n",
      "last_update_time_since_epoch: 1736852533882\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\ExampleValidator\\\\anomalies\\\\6\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}), exec_properties={'exclude_splits': '[]'}, execution_output_uri='./tfajarama-pipeline\\\\ExampleValidator\\\\.system\\\\executor_execution\\\\6\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\ExampleValidator\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\ExampleValidator\\\\.system\\\\executor_execution\\\\6\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "  }\n",
      "  id: \"ExampleValidator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.ExampleValidator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"statistics\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"StatisticsGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.StatisticsGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ExampleStatistics\"\n",
      "            base_type: STATISTICS\n",
      "          }\n",
      "        }\n",
      "        output_key: \"statistics\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"anomalies\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ExampleAnomalies\"\n",
      "          properties {\n",
      "            key: \"span\"\n",
      "            value: INT\n",
      "          }\n",
      "          properties {\n",
      "            key: \"split_names\"\n",
      "            value: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"exclude_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"[]\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"StatisticsGen\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "DEBUG:absl:Starting Executor execution.\n",
      "DEBUG:absl:Inputs for Executor are: {\"statistics\": [{\"artifact\": {\"id\": \"2\", \"type_id\": \"18\", \"uri\": \"./tfajarama-pipeline\\\\StatisticsGen\\\\statistics\\\\3\", \"properties\": {\"split_names\": {\"string_value\": \"[\\\"train\\\", \\\"eval\\\"]\"}}, \"custom_properties\": {\"state\": {\"string_value\": \"published\"}, \"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852532943\", \"last_update_time_since_epoch\": \"1736852532943\"}, \"artifact_type\": {\"id\": \"18\", \"name\": \"ExampleStatistics\", \"properties\": {\"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"STATISTICS\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ExampleStatistics\"}], \"schema\": [{\"artifact\": {\"id\": \"3\", \"type_id\": \"20\", \"uri\": \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\", \"custom_properties\": {\"tfx_version\": {\"string_value\": \"1.11.0\"}, \"state\": {\"string_value\": \"published\"}, \"is_external\": {\"int_value\": \"0\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852533882\", \"last_update_time_since_epoch\": \"1736852533882\"}, \"artifact_type\": {\"id\": \"20\", \"name\": \"Schema\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Schema\"}]}\n",
      "DEBUG:absl:Outputs for Executor are: {\"anomalies\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\ExampleValidator\\\\anomalies\\\\6\"}, \"artifact_type\": {\"name\": \"ExampleAnomalies\", \"properties\": {\"span\": \"INT\", \"split_names\": \"STRING\"}}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ExampleAnomalies\"}]}\n",
      "DEBUG:absl:Execution properties for Executor are: {\"exclude_splits\": \"[]\"}\n",
      "INFO:absl:Validating schema against the computed statistics for split train.\n",
      "INFO:absl:Validation complete for split train. Anomalies written to ./tfajarama-pipeline\\ExampleValidator\\anomalies\\6\\Split-train.\n",
      "INFO:absl:Validating schema against the computed statistics for split eval.\n",
      "INFO:absl:Validation complete for split eval. Anomalies written to ./tfajarama-pipeline\\ExampleValidator\\anomalies\\6\\Split-eval.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 6 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'anomalies': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\ExampleValidator\\\\anomalies\\\\6\"\n",
      ", artifact_type: name: \"ExampleAnomalies\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      ")]}) for execution 6\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 23.\n",
      "INFO:absl:node ExampleValidator is finished.\n",
      "INFO:absl:node Trainer is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.Trainer\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.Trainer\"\n",
      "  }\n",
      "}\n",
      " is 14.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Trainer] Resolved inputs: ({'examples': [Artifact(artifact: id: 9\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852665713\n",
      "last_update_time_since_epoch: 1736852665713\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'transform_graph': [Artifact(artifact: id: 4\n",
      "type_id: 22\n",
      "uri: \"./tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852665712\n",
      "last_update_time_since_epoch: 1736852665712\n",
      ", artifact_type: id: 22\n",
      "name: \"TransformGraph\"\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852533882\n",
      "last_update_time_since_epoch: 1736852533882\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 26.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 26\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"custom_config\"\n",
      "  value {\n",
      "    string_value: \"null\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"eval_args\"\n",
      "  value {\n",
      "    string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"module_path\"\n",
      "  value {\n",
      "    string_value: \"loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"train_args\"\n",
      "  value {\n",
      "    string_value: \"{\\n  \\\"num_steps\\\": 5000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "  }\n",
      "}\n",
      "name: \"318896e7-03d7-4d48-90da-9d5d25803dbd\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Trainer\\model_run\\7\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Trainer\\model\\7\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name b3df8cc99c6c60ecbe86b730c1f0e11c91637314472aeef260c6ec596a18d520\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"b3df8cc99c6c60ecbe86b730c1f0e11c91637314472aeef260c6ec596a18d520\"\n",
      "  }\n",
      "}\n",
      " is 15.\n",
      "INFO:absl:Going to run a new execution 7\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=7, input_dict={'examples': [Artifact(artifact: id: 9\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852665713\n",
      "last_update_time_since_epoch: 1736852665713\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")], 'transform_graph': [Artifact(artifact: id: 4\n",
      "type_id: 22\n",
      "uri: \"./tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852665712\n",
      "last_update_time_since_epoch: 1736852665712\n",
      ", artifact_type: id: 22\n",
      "name: \"TransformGraph\"\n",
      ")], 'schema': [Artifact(artifact: id: 3\n",
      "type_id: 20\n",
      "uri: \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852533882\n",
      "last_update_time_since_epoch: 1736852533882\n",
      ", artifact_type: id: 20\n",
      "name: \"Schema\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Trainer\\\\model_run\\\\7\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'eval_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'custom_config': 'null', 'module_path': 'loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 5000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'}, execution_output_uri='./tfajarama-pipeline\\\\Trainer\\\\.system\\\\executor_execution\\\\7\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\Trainer\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\Trainer\\\\.system\\\\executor_execution\\\\7\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.trainer.component.Trainer\"\n",
      "    base_type: TRAIN\n",
      "  }\n",
      "  id: \"Trainer\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Trainer\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transformed_examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"schema\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"SchemaGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.SchemaGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Schema\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"schema\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"transform_graph\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Transform\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Transform\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"TransformGraph\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"transform_graph\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"Model\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"model_run\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelRun\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"eval_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"module_path\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"train_args\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"num_steps\\\": 5000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"SchemaGen\"\n",
      "upstream_nodes: \"Transform\"\n",
      "downstream_nodes: \"Evaluator\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "DEBUG:absl:Starting GenericExecutor execution.\n",
      "DEBUG:absl:Inputs for GenericExecutor are: {\"examples\": [{\"artifact\": {\"id\": \"9\", \"type_id\": \"15\", \"uri\": \"./tfajarama-pipeline\\\\Transform\\\\transformed_examples\\\\5\", \"properties\": {\"split_names\": {\"string_value\": \"[\\\"train\\\", \\\"eval\\\"]\"}}, \"custom_properties\": {\"is_external\": {\"int_value\": \"0\"}, \"tfx_version\": {\"string_value\": \"1.11.0\"}, \"state\": {\"string_value\": \"published\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852665713\", \"last_update_time_since_epoch\": \"1736852665713\"}, \"artifact_type\": {\"id\": \"15\", \"name\": \"Examples\", \"properties\": {\"version\": \"INT\", \"split_names\": \"STRING\", \"span\": \"INT\"}, \"base_type\": \"DATASET\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Examples\"}], \"transform_graph\": [{\"artifact\": {\"id\": \"4\", \"type_id\": \"22\", \"uri\": \"./tfajarama-pipeline\\\\Transform\\\\transform_graph\\\\5\", \"custom_properties\": {\"state\": {\"string_value\": \"published\"}, \"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852665712\", \"last_update_time_since_epoch\": \"1736852665712\"}, \"artifact_type\": {\"id\": \"22\", \"name\": \"TransformGraph\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"TransformGraph\"}], \"schema\": [{\"artifact\": {\"id\": \"3\", \"type_id\": \"20\", \"uri\": \"./tfajarama-pipeline\\\\SchemaGen\\\\schema\\\\4\", \"custom_properties\": {\"tfx_version\": {\"string_value\": \"1.11.0\"}, \"state\": {\"string_value\": \"published\"}, \"is_external\": {\"int_value\": \"0\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852533882\", \"last_update_time_since_epoch\": \"1736852533882\"}, \"artifact_type\": {\"id\": \"20\", \"name\": \"Schema\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Schema\"}]}\n",
      "DEBUG:absl:Outputs for GenericExecutor are: {\"model_run\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Trainer\\\\model_run\\\\7\"}, \"artifact_type\": {\"name\": \"ModelRun\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ModelRun\"}], \"model\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"}, \"artifact_type\": {\"name\": \"Model\", \"base_type\": \"MODEL\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Model\"}]}\n",
      "DEBUG:absl:Execution properties for GenericExecutor are: {\"eval_args\": \"{\\n  \\\"num_steps\\\": 1000,\\n  \\\"splits\\\": [\\n    \\\"eval\\\"\\n  ]\\n}\", \"custom_config\": \"null\", \"module_path\": \"loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl\", \"train_args\": \"{\\n  \\\"num_steps\\\": 5000,\\n  \\\"splits\\\": [\\n    \\\"train\\\"\\n  ]\\n}\"}\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
      "INFO:absl:udf_utils.get_fn {'eval_args': '{\\n  \"num_steps\": 1000,\\n  \"splits\": [\\n    \"eval\"\\n  ]\\n}', 'custom_config': 'null', 'module_path': 'loan_approval_trainer@./tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl', 'train_args': '{\\n  \"num_steps\": 5000,\\n  \"splits\": [\\n    \"train\"\\n  ]\\n}'} 'run_fn'\n",
      "INFO:absl:Installing './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl' to a temporary directory.\n",
      "INFO:absl:Executing: ['D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\Scripts\\\\python.exe', '-m', 'pip', 'install', '--target', 'C:\\\\Users\\\\Asus\\\\AppData\\\\Local\\\\Temp\\\\tmpwadf5yn7', './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl']\n",
      "INFO:absl:Successfully installed './tfajarama-pipeline\\\\_wheels\\\\tfx_user_code_Trainer-0.0+966723c8b13346963e08853a749e14f739ca1d67e4be7c845c57f88a8ae56359-py3-none-any.whl'.\n",
      "INFO:absl:Training model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " person_gender_xf (InputLayer)  [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " person_education_xf (InputLaye  [(None, 6)]         0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " person_home_ownership_xf (Inpu  [(None, 5)]         0           []                               \n",
      " tLayer)                                                                                          \n",
      "                                                                                                  \n",
      " loan_intent_xf (InputLayer)    [(None, 6)]          0           []                               \n",
      "                                                                                                  \n",
      " previous_loan_defaults_on_file  [(None, 3)]         0           []                               \n",
      " _xf (InputLayer)                                                                                 \n",
      "                                                                                                  \n",
      " person_age_xf (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " person_income_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " person_emp_exp_xf (InputLayer)  [(None, 1)]         0           []                               \n",
      "                                                                                                  \n",
      " loan_amnt_xf (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " loan_int_rate_xf (InputLayer)  [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " loan_percent_income_xf (InputL  [(None, 1)]         0           []                               \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " cb_person_cred_hist_length_xf   [(None, 1)]         0           []                               \n",
      " (InputLayer)                                                                                     \n",
      "                                                                                                  \n",
      " credit_score_xf (InputLayer)   [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 31)           0           ['person_gender_xf[0][0]',       \n",
      "                                                                  'person_education_xf[0][0]',    \n",
      "                                                                  'person_home_ownership_xf[0][0]'\n",
      "                                                                 , 'loan_intent_xf[0][0]',        \n",
      "                                                                  'previous_loan_defaults_on_file_\n",
      "                                                                 xf[0][0]',                       \n",
      "                                                                  'person_age_xf[0][0]',          \n",
      "                                                                  'person_income_xf[0][0]',       \n",
      "                                                                  'person_emp_exp_xf[0][0]',      \n",
      "                                                                  'loan_amnt_xf[0][0]',           \n",
      "                                                                  'loan_int_rate_xf[0][0]',       \n",
      "                                                                  'loan_percent_income_xf[0][0]', \n",
      "                                                                  'cb_person_cred_hist_length_xf[0\n",
      "                                                                 ][0]',                           \n",
      "                                                                  'credit_score_xf[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          4096        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           8256        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            33          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 14,465\n",
      "Trainable params: 14,465\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "5000/5000 [==============================] - 16s 3ms/step - loss: 0.2553 - binary_accuracy: 0.8805 - val_loss: 0.2241 - val_binary_accuracy: 0.8961\n",
      "Epoch 2/10\n",
      "5000/5000 [==============================] - 13s 3ms/step - loss: 0.2127 - binary_accuracy: 0.9011 - val_loss: 0.2219 - val_binary_accuracy: 0.8958\n",
      "Epoch 3/10\n",
      "5000/5000 [==============================] - 15s 3ms/step - loss: 0.2070 - binary_accuracy: 0.9047 - val_loss: 0.2146 - val_binary_accuracy: 0.9027\n",
      "Epoch 4/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.2011 - binary_accuracy: 0.9078 - val_loss: 0.2092 - val_binary_accuracy: 0.9050\n",
      "Epoch 5/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1946 - binary_accuracy: 0.9121 - val_loss: 0.2051 - val_binary_accuracy: 0.9078\n",
      "Epoch 6/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1884 - binary_accuracy: 0.9152 - val_loss: 0.2009 - val_binary_accuracy: 0.9082\n",
      "Epoch 7/10\n",
      "5000/5000 [==============================] - 13s 3ms/step - loss: 0.1836 - binary_accuracy: 0.9176 - val_loss: 0.1953 - val_binary_accuracy: 0.9103\n",
      "Epoch 8/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1799 - binary_accuracy: 0.9185 - val_loss: 0.1965 - val_binary_accuracy: 0.9106\n",
      "Epoch 9/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1786 - binary_accuracy: 0.9194 - val_loss: 0.1944 - val_binary_accuracy: 0.9108\n",
      "Epoch 10/10\n",
      "5000/5000 [==============================] - 14s 3ms/step - loss: 0.1763 - binary_accuracy: 0.9203 - val_loss: 0.1935 - val_binary_accuracy: 0.9117\n",
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:struct2tensor is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_decision_forests is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tensorflow_text is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tfajarama-pipeline\\Trainer\\model\\7\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tfajarama-pipeline\\Trainer\\model\\7\\Format-Serving\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Training complete. Model written to ./tfajarama-pipeline\\Trainer\\model\\7\\Format-Serving. ModelRun written to ./tfajarama-pipeline\\Trainer\\model_run\\7\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 7 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'model_run': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Trainer\\\\model_run\\\\7\"\n",
      ", artifact_type: name: \"ModelRun\"\n",
      ")], 'model': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      ", artifact_type: name: \"Model\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 7\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 27.\n",
      "DEBUG:absl:Registering a metadata type with id 28.\n",
      "INFO:absl:node Trainer is finished.\n",
      "INFO:absl:node Evaluator is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"loan_status\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.Evaluator\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.Evaluator\"\n",
      "  }\n",
      "}\n",
      " is 16.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Evaluator] Resolved inputs: ({'model': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852817466\n",
      "last_update_time_since_epoch: 1736852817466\n",
      ", artifact_type: id: 28\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'baseline_model': [], 'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852477657\n",
      "last_update_time_since_epoch: 1736852477657\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 29.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 29\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"eval_config\"\n",
      "  value {\n",
      "    string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"loan_status\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"example_splits\"\n",
      "  value {\n",
      "    string_value: \"null\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"fairness_indicator_thresholds\"\n",
      "  value {\n",
      "    string_value: \"null\"\n",
      "  }\n",
      "}\n",
      "name: \"f2d5236e-843a-4e93-a04d-867b9f9a97e3\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Evaluator\\evaluation\\8\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Evaluator\\blessing\\8\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name b08c8212b95adcbfc88a9278d1b868497792f3f4615135e21a6b5515c1510fea\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"b08c8212b95adcbfc88a9278d1b868497792f3f4615135e21a6b5515c1510fea\"\n",
      "  }\n",
      "}\n",
      " is 17.\n",
      "INFO:absl:Going to run a new execution 8\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=8, input_dict={'model': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852817466\n",
      "last_update_time_since_epoch: 1736852817466\n",
      ", artifact_type: id: 28\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'baseline_model': [], 'examples': [Artifact(artifact: id: 1\n",
      "type_id: 15\n",
      "uri: \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\"\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value {\n",
      "    string_value: \"[\\\"train\\\", \\\"eval\\\"]\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"file_format\"\n",
      "  value {\n",
      "    string_value: \"tfrecords_gzip\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"input_fingerprint\"\n",
      "  value {\n",
      "    string_value: \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"payload_format\"\n",
      "  value {\n",
      "    string_value: \"FORMAT_TF_EXAMPLE\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"span\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852477657\n",
      "last_update_time_since_epoch: 1736852477657\n",
      ", artifact_type: id: 15\n",
      "name: \"Examples\"\n",
      "properties {\n",
      "  key: \"span\"\n",
      "  value: INT\n",
      "}\n",
      "properties {\n",
      "  key: \"split_names\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "base_type: DATASET\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Evaluator\\\\evaluation\\\\8\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Evaluator\\\\blessing\\\\8\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}), exec_properties={'example_splits': 'null', 'fairness_indicator_thresholds': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"loan_status\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'}, execution_output_uri='./tfajarama-pipeline\\\\Evaluator\\\\.system\\\\executor_execution\\\\8\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\Evaluator\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\Evaluator\\\\.system\\\\executor_execution\\\\8\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "    base_type: EVALUATE\n",
      "  }\n",
      "  id: \"Evaluator\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Evaluator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"baseline_model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Latest_blessed_model_resolver\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Latest_blessed_model_resolver\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"examples\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"CsvExampleGen\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.CsvExampleGen\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Examples\"\n",
      "            base_type: DATASET\n",
      "          }\n",
      "        }\n",
      "        output_key: \"examples\"\n",
      "      }\n",
      "      min_count: 1\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"blessing\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelBlessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  outputs {\n",
      "    key: \"evaluation\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"ModelEvaluation\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"eval_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"loan_status\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"example_splits\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"fairness_indicator_thresholds\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"CsvExampleGen\"\n",
      "upstream_nodes: \"Latest_blessed_model_resolver\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "downstream_nodes: \"Pusher\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "INFO:absl:Attempting to infer TFX Python dependency for beam\n",
      "INFO:absl:Copying all content from install dir D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tfx to temp dir C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdhio1pma\\build\\tfx\n",
      "INFO:absl:Generating a temp setup file at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdhio1pma\\build\\tfx\\setup.py\n",
      "INFO:absl:Creating temporary sdist package, logs available at C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdhio1pma\\build\\tfx\\setup.log\n",
      "INFO:absl:Added --extra_package=C:\\Users\\Asus\\AppData\\Local\\Temp\\tmpdhio1pma\\build\\tfx\\dist\\tfx_ephemeral-1.11.0.tar.gz to beam args\n",
      "DEBUG:absl:Starting Executor execution.\n",
      "DEBUG:absl:Inputs for Executor are: {\"model\": [{\"artifact\": {\"id\": \"14\", \"type_id\": \"28\", \"uri\": \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\", \"custom_properties\": {\"state\": {\"string_value\": \"published\"}, \"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852817466\", \"last_update_time_since_epoch\": \"1736852817466\"}, \"artifact_type\": {\"id\": \"28\", \"name\": \"Model\", \"base_type\": \"MODEL\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Model\"}], \"baseline_model\": [], \"examples\": [{\"artifact\": {\"id\": \"1\", \"type_id\": \"15\", \"uri\": \"./tfajarama-pipeline\\\\CsvExampleGen\\\\examples\\\\1\", \"properties\": {\"split_names\": {\"string_value\": \"[\\\"train\\\", \\\"eval\\\"]\"}}, \"custom_properties\": {\"tfx_version\": {\"string_value\": \"1.11.0\"}, \"state\": {\"string_value\": \"published\"}, \"span\": {\"int_value\": \"0\"}, \"payload_format\": {\"string_value\": \"FORMAT_TF_EXAMPLE\"}, \"input_fingerprint\": {\"string_value\": \"split:single_split,num_files:1,total_bytes:3610978,xor_checksum:1734719350,sum_checksum:1734719350\"}, \"file_format\": {\"string_value\": \"tfrecords_gzip\"}, \"is_external\": {\"int_value\": \"0\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852477657\", \"last_update_time_since_epoch\": \"1736852477657\"}, \"artifact_type\": {\"id\": \"15\", \"name\": \"Examples\", \"properties\": {\"split_names\": \"STRING\", \"span\": \"INT\", \"version\": \"INT\"}, \"base_type\": \"DATASET\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Examples\"}]}\n",
      "DEBUG:absl:Outputs for Executor are: {\"evaluation\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Evaluator\\\\evaluation\\\\8\"}, \"artifact_type\": {\"name\": \"ModelEvaluation\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ModelEvaluation\"}], \"blessing\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Evaluator\\\\blessing\\\\8\"}, \"artifact_type\": {\"name\": \"ModelBlessing\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ModelBlessing\"}]}\n",
      "DEBUG:absl:Execution properties for Executor are: {\"example_splits\": \"null\", \"fairness_indicator_thresholds\": \"null\", \"eval_config\": \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"AUC\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Precision\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"Recall\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"ExampleCount\\\"\\n        },\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": 0.0001,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"loan_status\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"}\n",
      "INFO:absl:udf_utils.get_fn {'example_splits': 'null', 'fairness_indicator_thresholds': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"loan_status\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'} 'custom_eval_shared_model'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"loan_status\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:Using ./tfajarama-pipeline\\Trainer\\model\\7\\Format-Serving as  model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001CFB18B6C70> and <keras.engine.input_layer.InputLayer object at 0x000001CFAFF35AF0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001CFB18B6C70> and <keras.engine.input_layer.InputLayer object at 0x000001CFAFF35AF0>).\n",
      "INFO:absl:The 'example_splits' parameter is not set, using 'eval' split.\n",
      "INFO:absl:Evaluating model.\n",
      "INFO:absl:udf_utils.get_fn {'example_splits': 'null', 'fairness_indicator_thresholds': 'null', 'eval_config': '{\\n  \"metrics_specs\": [\\n    {\\n      \"metrics\": [\\n        {\\n          \"class_name\": \"AUC\"\\n        },\\n        {\\n          \"class_name\": \"Precision\"\\n        },\\n        {\\n          \"class_name\": \"Recall\"\\n        },\\n        {\\n          \"class_name\": \"ExampleCount\"\\n        },\\n        {\\n          \"class_name\": \"BinaryAccuracy\",\\n          \"threshold\": {\\n            \"change_threshold\": {\\n              \"absolute\": 0.0001,\\n              \"direction\": \"HIGHER_IS_BETTER\"\\n            },\\n            \"value_threshold\": {\\n              \"lower_bound\": 0.5\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \"model_specs\": [\\n    {\\n      \"label_key\": \"loan_status\"\\n    }\\n  ],\\n  \"slicing_specs\": [\\n    {}\\n  ]\\n}'} 'custom_extractors'\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"loan_status\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"loan_status\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n",
      "INFO:absl:Request was made to ignore the baseline ModelSpec and any change thresholds. This is likely because a baseline model was not provided: updated_config=\n",
      "model_specs {\n",
      "  label_key: \"loan_status\"\n",
      "}\n",
      "slicing_specs {\n",
      "}\n",
      "metrics_specs {\n",
      "  metrics {\n",
      "    class_name: \"AUC\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Precision\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"Recall\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"ExampleCount\"\n",
      "  }\n",
      "  metrics {\n",
      "    class_name: \"BinaryAccuracy\"\n",
      "    threshold {\n",
      "      value_threshold {\n",
      "        lower_bound {\n",
      "          value: 0.5\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  model_names: \"\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001CFADFF14C0> and <keras.engine.input_layer.InputLayer object at 0x000001CFAE002CA0>).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001CFADFF14C0> and <keras.engine.input_layer.InputLayer object at 0x000001CFAE002CA0>).\n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 60268640 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 74268579 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 388996839 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 420519351 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 626596212 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 638594388 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 780221462 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 795231342 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 791232824 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 809755086 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 871758699 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 884754896 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 878754854 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 891765594 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 955291032 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\runners\\\\worker\\\\sdk_worker_main.py:339\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852851   nanos: 969290256 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\options\\\\pipeline_options.py:356\" thread: \"MainThread\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 311353206 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FEC69BE430> and <keras.engine.input_layer.InputLayer object at 0x000001FEC65A34F0>).\" instruction_id: \"bundle_2386\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 539981603 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002173C5F62E0> and <keras.engine.input_layer.InputLayer object at 0x000002173C1B2550>).\" instruction_id: \"bundle_2387\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 562983512 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263E16AB520> and <keras.engine.input_layer.InputLayer object at 0x00000263E155A250>).\" instruction_id: \"bundle_2384\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 608993768 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000026040E49400> and <keras.engine.input_layer.InputLayer object at 0x0000026040CDE6A0>).\" instruction_id: \"bundle_2388\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 608993768 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202EB9A13D0> and <keras.engine.input_layer.InputLayer object at 0x00000202E4833F40>).\" instruction_id: \"bundle_2390\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 653522491 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F48731B4F0> and <keras.engine.input_layer.InputLayer object at 0x000001F4871E1730>).\" instruction_id: \"bundle_2385\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 669522523 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000200281BB760> and <keras.engine.input_layer.InputLayer object at 0x00000200280826D0>).\" instruction_id: \"bundle_2389\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852868   nanos: 703536272 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000169957CBD90> and <keras.engine.input_layer.InputLayer object at 0x0000016995782820>).\" instruction_id: \"bundle_2391\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852870   nanos: 793738126 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FED0EEC4C0> and <keras.engine.input_layer.InputLayer object at 0x000001FED0EBFA30>).\" instruction_id: \"bundle_2386\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 191442966 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000021746AFA520> and <keras.engine.input_layer.InputLayer object at 0x0000021746AD3A90>).\" instruction_id: \"bundle_2387\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 224983692 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263EAC338E0> and <keras.engine.input_layer.InputLayer object at 0x00000263EAC09E50>).\" instruction_id: \"bundle_2384\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 267981767 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202EDECF430> and <keras.engine.input_layer.InputLayer object at 0x00000202EDEA29A0>).\" instruction_id: \"bundle_2390\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 277982711 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2386\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 251983165 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002604B32A430> and <keras.engine.input_layer.InputLayer object at 0x000002604B2FF9A0>).\" instruction_id: \"bundle_2388\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 290982246 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F49084B4C0> and <keras.engine.input_layer.InputLayer object at 0x000001F490820A30>).\" instruction_id: \"bundle_2385\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 414038896 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000200326FF3D0> and <keras.engine.input_layer.InputLayer object at 0x00000200326D2940>).\" instruction_id: \"bundle_2389\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 432051420 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001699EE23DF0> and <keras.engine.input_layer.InputLayer object at 0x000001699EDB2F70>).\" instruction_id: \"bundle_2391\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 737675428 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2390\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 738676309 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2387\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 770674943 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2384\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 806689262 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2385\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 806689262 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2388\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 897221326 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2389\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852871   nanos: 944762229 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_2391\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\apache_beam\\\\io\\\\tfrecordio.py:59\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 154869318 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FED662C310> and <keras.engine.input_layer.InputLayer object at 0x000001FED65B15B0>).\" instruction_id: \"bundle_2434\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 451509952 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F494E8FF70> and <keras.engine.input_layer.InputLayer object at 0x000001F492D5D7F0>).\" instruction_id: \"bundle_2433\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 465508460 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202F3518460> and <keras.engine.input_layer.InputLayer object at 0x00000202F356D310>).\" instruction_id: \"bundle_2438\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 469509840 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002174C18E580> and <keras.engine.input_layer.InputLayer object at 0x000002174C1D6490>).\" instruction_id: \"bundle_2435\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 494508743 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263F0209B50> and <keras.engine.input_layer.InputLayer object at 0x00000263F022C940>).\" instruction_id: \"bundle_2432\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 493509054 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000169A44DDFD0> and <keras.engine.input_layer.InputLayer object at 0x00000169A44C0AC0>).\" instruction_id: \"bundle_2439\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 505526065 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000020037CEA610> and <keras.engine.input_layer.InputLayer object at 0x0000020037D17190>).\" instruction_id: \"bundle_2437\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852877   nanos: 547046422 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000026050979C10> and <keras.engine.input_layer.InputLayer object at 0x000002605098F640>).\" instruction_id: \"bundle_2436\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852879   nanos: 593774080 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FED8988310> and <keras.engine.input_layer.InputLayer object at 0x000001FED33FE0D0>).\" instruction_id: \"bundle_2434\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 89851617 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F4982B8910> and <keras.engine.input_layer.InputLayer object at 0x000001F498259940>).\" instruction_id: \"bundle_2433\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 102851152 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000169A6896BE0> and <keras.engine.input_layer.InputLayer object at 0x00000169A685F910>).\" instruction_id: \"bundle_2439\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 95850467 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202F5930F70> and <keras.engine.input_layer.InputLayer object at 0x00000202F34EF760>).\" instruction_id: \"bundle_2438\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 112863540 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263F26A7100> and <keras.engine.input_layer.InputLayer object at 0x00000263F022CF70>).\" instruction_id: \"bundle_2432\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 138389110 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000026052D94190> and <keras.engine.input_layer.InputLayer object at 0x000002605097F970>).\" instruction_id: \"bundle_2436\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 158812999 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002003A166430> and <keras.engine.input_layer.InputLayer object at 0x000002003A0FAA00>).\" instruction_id: \"bundle_2437\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852880   nanos: 203373670 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002174E5D4370> and <keras.engine.input_layer.InputLayer object at 0x00000217490849A0>).\" instruction_id: \"bundle_2435\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 22594213 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FEDADFEF70> and <keras.engine.input_layer.InputLayer object at 0x000001FEDADD55B0>).\" instruction_id: \"bundle_2458\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 183121204 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000026055298550> and <keras.engine.input_layer.InputLayer object at 0x0000026055267AC0>).\" instruction_id: \"bundle_2460\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 220151901 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000021750A38040> and <keras.engine.input_layer.InputLayer object at 0x0000021750A8FA30>).\" instruction_id: \"bundle_2459\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 227679729 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202F7E3BCA0> and <keras.engine.input_layer.InputLayer object at 0x00000202F7E08EB0>).\" instruction_id: \"bundle_2462\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 260682821 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000169A8DA8FA0> and <keras.engine.input_layer.InputLayer object at 0x00000169A8D7C550>).\" instruction_id: \"bundle_2463\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 312697887 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263F4B4EEE0> and <keras.engine.input_layer.InputLayer object at 0x00000263F4B23670>).\" instruction_id: \"bundle_2456\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 315693378 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F49A7BBCD0> and <keras.engine.input_layer.InputLayer object at 0x000001F49A78DE50>).\" instruction_id: \"bundle_2457\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852883   nanos: 358223199 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002003C6757F0> and <keras.engine.input_layer.InputLayer object at 0x000002003C643D60>).\" instruction_id: \"bundle_2461\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 332510948 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FEDD3F4EE0> and <keras.engine.input_layer.InputLayer object at 0x000001FEDD2E80D0>).\" instruction_id: \"bundle_2482\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 392043828 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002175300F820> and <keras.engine.input_layer.InputLayer object at 0x0000021752FD9D90>).\" instruction_id: \"bundle_2483\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 416056632 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202FA3E1F10> and <keras.engine.input_layer.InputLayer object at 0x00000202FA2E61F0>).\" instruction_id: \"bundle_2486\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 449582099 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002605783DFA0> and <keras.engine.input_layer.InputLayer object at 0x0000026057752250>).\" instruction_id: \"bundle_2484\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 463582992 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F49CD64BE0> and <keras.engine.input_layer.InputLayer object at 0x000001F49CCE1F10>).\" instruction_id: \"bundle_2481\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 464582920 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000169AB3416A0> and <keras.engine.input_layer.InputLayer object at 0x00000169AB309940>).\" instruction_id: \"bundle_2487\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 486582279 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263C6CFAE20> and <keras.engine.input_layer.InputLayer object at 0x00000263F70AE0D0>).\" instruction_id: \"bundle_2480\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852886   nanos: 498582363 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000002003FBE8FA0> and <keras.engine.input_layer.InputLayer object at 0x000002003EB1B280>).\" instruction_id: \"bundle_2485\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852888   nanos: 982113599 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001FEDF9A3D90> and <keras.engine.input_layer.InputLayer object at 0x000001FEDD3143D0>).\" instruction_id: \"bundle_2482\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-12\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 375799655 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000169AE7A4B50> and <keras.engine.input_layer.InputLayer object at 0x00000169AB320B80>).\" instruction_id: \"bundle_2487\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 392799854 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000021755462D60> and <keras.engine.input_layer.InputLayer object at 0x0000021752FF1D00>).\" instruction_id: \"bundle_2483\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 413810014 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000026059C9B5E0> and <keras.engine.input_layer.InputLayer object at 0x0000026059C0A5B0>).\" instruction_id: \"bundle_2484\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 421808242 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x000001F49F1B6A90> and <keras.engine.input_layer.InputLayer object at 0x000001F49F160550>).\" instruction_id: \"bundle_2481\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 527872562 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x0000020042048490> and <keras.engine.input_layer.InputLayer object at 0x000002003FBD04C0>).\" instruction_id: \"bundle_2485\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-10\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 568872213 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000202FC842400> and <keras.engine.input_layer.InputLayer object at 0x00000202FA2FA430>).\" instruction_id: \"bundle_2486\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1736852889   nanos: 578872680 } message: \"Inconsistent references when loading the checkpoint into this object graph. For example, in the saved checkpoint object, `model.layer.weight` and `model.layer_copy.weight` reference the same variable, while in the current object these are two different variables. The referenced variables are:(<keras.saving.saved_model.load.TensorFlowTransform>TransformFeaturesLayer object at 0x00000263FA54A5B0> and <keras.engine.input_layer.InputLayer object at 0x00000263F70AEE50>).\" instruction_id: \"bundle_2480\" log_location: \"D:\\\\AI-ML_Project\\\\machine-learning-operations-mlops\\\\submissions\\\\submission-2-root\\\\.venv\\\\lib\\\\site-packages\\\\tensorflow\\\\python\\\\checkpoint\\\\restore.py:84\" thread: \"Thread-13\" \n",
      "INFO:absl:Evaluation complete. Results written to ./tfajarama-pipeline\\Evaluator\\evaluation\\8.\n",
      "INFO:absl:Checking validation results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AI-ML_Project\\machine-learning-operations-mlops\\submissions\\submission-2-root\\.venv\\lib\\site-packages\\tensorflow_model_analysis\\writers\\metrics_plots_and_validations_writer.py:110: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n",
      "INFO:absl:Blessing result True written to ./tfajarama-pipeline\\Evaluator\\blessing\\8.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 8 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'evaluation': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Evaluator\\\\evaluation\\\\8\"\n",
      ", artifact_type: name: \"ModelEvaluation\"\n",
      ")], 'blessing': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Evaluator\\\\blessing\\\\8\"\n",
      ", artifact_type: name: \"ModelBlessing\"\n",
      ")]}) for execution 8\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 30.\n",
      "DEBUG:absl:Registering a metadata type with id 31.\n",
      "INFO:absl:node Evaluator is finished.\n",
      "INFO:absl:node Pusher is running.\n",
      "INFO:absl:Running launcher for node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      "\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type node and name tfajarama-pipeline.Pusher\n",
      "DEBUG:absl:Registering a metadata type with id 12.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"node\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"tfajarama-pipeline.Pusher\"\n",
      "  }\n",
      "}\n",
      " is 18.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
      "INFO:absl:[Pusher] Resolved inputs: ({'model': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852817466\n",
      "last_update_time_since_epoch: 1736852817466\n",
      ", artifact_type: id: 28\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 16\n",
      "type_id: 31\n",
      "uri: \"./tfajarama-pipeline\\\\Evaluator\\\\blessing\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 14\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852902361\n",
      "last_update_time_since_epoch: 1736852902361\n",
      ", artifact_type: id: 31\n",
      "name: \"ModelBlessing\"\n",
      ")]},)\n",
      "DEBUG:absl:Registering a metadata type with id 32.\n",
      "DEBUG:absl:Prepared EXECUTION:\n",
      " type_id: 32\n",
      "last_known_state: RUNNING\n",
      "custom_properties {\n",
      "  key: \"custom_config\"\n",
      "  value {\n",
      "    string_value: \"null\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"push_destination\"\n",
      "  value {\n",
      "    string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\"\\n  }\\n}\"\n",
      "  }\n",
      "}\n",
      "name: \"7220b2e7-9621-43b1-a824-ff01da5121d1\"\n",
      "\n",
      "DEBUG:absl:Creating output artifact uri ./tfajarama-pipeline\\Pusher\\pushed_model\\9\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Failed to get context of type execution_cache and name 577fb3336a0e8ac37ccf0c22df1e74435f58719f0e926ebd600f538cf066f6be\n",
      "DEBUG:absl:Registering a metadata type with id 14.\n",
      "DEBUG:absl:ID of context type {\n",
      "  name: \"execution_cache\"\n",
      "}\n",
      "name {\n",
      "  field_value {\n",
      "    string_value: \"577fb3336a0e8ac37ccf0c22df1e74435f58719f0e926ebd600f538cf066f6be\"\n",
      "  }\n",
      "}\n",
      " is 19.\n",
      "INFO:absl:Going to run a new execution 9\n",
      "INFO:absl:Going to run a new execution: ExecutionInfo(execution_id=9, input_dict={'model': [Artifact(artifact: id: 14\n",
      "type_id: 28\n",
      "uri: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852817466\n",
      "last_update_time_since_epoch: 1736852817466\n",
      ", artifact_type: id: 28\n",
      "name: \"Model\"\n",
      "base_type: MODEL\n",
      ")], 'model_blessing': [Artifact(artifact: id: 16\n",
      "type_id: 31\n",
      "uri: \"./tfajarama-pipeline\\\\Evaluator\\\\blessing\\\\8\"\n",
      "custom_properties {\n",
      "  key: \"blessed\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model\"\n",
      "  value {\n",
      "    string_value: \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"current_model_id\"\n",
      "  value {\n",
      "    int_value: 14\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"is_external\"\n",
      "  value {\n",
      "    int_value: 0\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"published\"\n",
      "  }\n",
      "}\n",
      "custom_properties {\n",
      "  key: \"tfx_version\"\n",
      "  value {\n",
      "    string_value: \"1.11.0\"\n",
      "  }\n",
      "}\n",
      "state: LIVE\n",
      "create_time_since_epoch: 1736852902361\n",
      "last_update_time_since_epoch: 1736852902361\n",
      ", artifact_type: id: 31\n",
      "name: \"ModelBlessing\"\n",
      ")]}, output_dict=defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Pusher\\\\pushed_model\\\\9\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}), exec_properties={'custom_config': 'null', 'push_destination': '{\\n  \"filesystem\": {\\n    \"base_directory\": \"./serving_model\"\\n  }\\n}'}, execution_output_uri='./tfajarama-pipeline\\\\Pusher\\\\.system\\\\executor_execution\\\\9\\\\executor_output.pb', stateful_working_dir='./tfajarama-pipeline\\\\Pusher\\\\.system\\\\stateful_working_dir\\\\20250114-180011.700094', tmp_dir='./tfajarama-pipeline\\\\Pusher\\\\.system\\\\executor_execution\\\\9\\\\.temp\\\\', pipeline_node=node_info {\n",
      "  type {\n",
      "    name: \"tfx.components.pusher.component.Pusher\"\n",
      "    base_type: DEPLOY\n",
      "  }\n",
      "  id: \"Pusher\"\n",
      "}\n",
      "contexts {\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"pipeline_run\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"20250114-180011.700094\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  contexts {\n",
      "    type {\n",
      "      name: \"node\"\n",
      "    }\n",
      "    name {\n",
      "      field_value {\n",
      "        string_value: \"tfajarama-pipeline.Pusher\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  inputs {\n",
      "    key: \"model\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Trainer\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Trainer\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"Model\"\n",
      "            base_type: MODEL\n",
      "          }\n",
      "        }\n",
      "        output_key: \"model\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  inputs {\n",
      "    key: \"model_blessing\"\n",
      "    value {\n",
      "      channels {\n",
      "        producer_node_query {\n",
      "          id: \"Evaluator\"\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"pipeline_run\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"20250114-180011.700094\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        context_queries {\n",
      "          type {\n",
      "            name: \"node\"\n",
      "          }\n",
      "          name {\n",
      "            field_value {\n",
      "              string_value: \"tfajarama-pipeline.Evaluator\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        artifact_query {\n",
      "          type {\n",
      "            name: \"ModelBlessing\"\n",
      "          }\n",
      "        }\n",
      "        output_key: \"blessing\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  outputs {\n",
      "    key: \"pushed_model\"\n",
      "    value {\n",
      "      artifact_spec {\n",
      "        type {\n",
      "          name: \"PushedModel\"\n",
      "          base_type: MODEL\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "parameters {\n",
      "  parameters {\n",
      "    key: \"custom_config\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"null\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  parameters {\n",
      "    key: \"push_destination\"\n",
      "    value {\n",
      "      field_value {\n",
      "        string_value: \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\"\\n  }\\n}\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "upstream_nodes: \"Evaluator\"\n",
      "upstream_nodes: \"Trainer\"\n",
      "execution_options {\n",
      "  caching_options {\n",
      "    enable_cache: true\n",
      "  }\n",
      "}\n",
      ", pipeline_info=id: \"tfajarama-pipeline\"\n",
      ", pipeline_run_id='20250114-180011.700094')\n",
      "DEBUG:absl:Starting Executor execution.\n",
      "DEBUG:absl:Inputs for Executor are: {\"model\": [{\"artifact\": {\"id\": \"14\", \"type_id\": \"28\", \"uri\": \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\", \"custom_properties\": {\"tfx_version\": {\"string_value\": \"1.11.0\"}, \"is_external\": {\"int_value\": \"0\"}, \"state\": {\"string_value\": \"published\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852817466\", \"last_update_time_since_epoch\": \"1736852817466\"}, \"artifact_type\": {\"id\": \"28\", \"name\": \"Model\", \"base_type\": \"MODEL\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"Model\"}], \"model_blessing\": [{\"artifact\": {\"id\": \"16\", \"type_id\": \"31\", \"uri\": \"./tfajarama-pipeline\\\\Evaluator\\\\blessing\\\\8\", \"custom_properties\": {\"tfx_version\": {\"string_value\": \"1.11.0\"}, \"blessed\": {\"int_value\": \"1\"}, \"state\": {\"string_value\": \"published\"}, \"is_external\": {\"int_value\": \"0\"}, \"current_model_id\": {\"int_value\": \"14\"}, \"current_model\": {\"string_value\": \"./tfajarama-pipeline\\\\Trainer\\\\model\\\\7\"}}, \"state\": \"LIVE\", \"create_time_since_epoch\": \"1736852902361\", \"last_update_time_since_epoch\": \"1736852902361\"}, \"artifact_type\": {\"id\": \"31\", \"name\": \"ModelBlessing\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"ModelBlessing\"}]}\n",
      "DEBUG:absl:Outputs for Executor are: {\"pushed_model\": [{\"artifact\": {\"uri\": \"./tfajarama-pipeline\\\\Pusher\\\\pushed_model\\\\9\"}, \"artifact_type\": {\"name\": \"PushedModel\", \"base_type\": \"MODEL\"}, \"__artifact_class_module__\": \"tfx.types.standard_artifacts\", \"__artifact_class_name__\": \"PushedModel\"}]}\n",
      "DEBUG:absl:Execution properties for Executor are: {\"custom_config\": \"null\", \"push_destination\": \"{\\n  \\\"filesystem\\\": {\\n    \\\"base_directory\\\": \\\"./serving_model\\\"\\n  }\\n}\"}\n",
      "INFO:absl:Model version: 1736852903\n",
      "INFO:absl:Model written to serving path ./serving_model\\1736852903.\n",
      "INFO:absl:Model pushed to ./tfajarama-pipeline\\Pusher\\pushed_model\\9.\n",
      "INFO:absl:Cleaning up stateless execution info.\n",
      "INFO:absl:Execution 9 succeeded.\n",
      "INFO:absl:Cleaning up stateful execution info.\n",
      "INFO:absl:Publishing output artifacts defaultdict(<class 'list'>, {'pushed_model': [Artifact(artifact: uri: \"./tfajarama-pipeline\\\\Pusher\\\\pushed_model\\\\9\"\n",
      ", artifact_type: name: \"PushedModel\"\n",
      "base_type: MODEL\n",
      ")]}) for execution 9\n",
      "INFO:absl:MetadataStore with DB connection initialized\n",
      "DEBUG:absl:ConnectionConfig: sqlite {\n",
      "  filename_uri: \"./tfajarama-pipeline\\\\metadata.sqlite\"\n",
      "  connection_mode: READWRITE_OPENCREATE\n",
      "}\n",
      "\n",
      "DEBUG:absl:Registering a metadata type with id 33.\n",
      "INFO:absl:node Pusher is finished.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # logging.set_verbosity(logging.INFO)\n",
    "    logging.set_verbosity(logging.DEBUG)\n",
    "    logging.info(\"Starting the pipeline...\")\n",
    "    \n",
    "    from modules.components import init_components\n",
    "    \n",
    "    components = init_components(\n",
    "        DATA_ROOT,\n",
    "        transform_module=TRANSFORM_MODULE_FILE,\n",
    "        # tuning_module=TUNER_MODULE_FILE,\n",
    "        training_module=TRAINER_MODULE_FILE,\n",
    "        training_steps=5000,\n",
    "        eval_steps=1000,\n",
    "        serving_model_dir=serving_model_dir,\n",
    "    )\n",
    "    \n",
    "    pipeline = init_local_pipeline(components, pipeline_root)\n",
    "    BeamDagRunner().run(pipeline=pipeline)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T11:08:24.388847Z",
     "start_time": "2025-01-14T11:00:08.316947Z"
    }
   },
   "id": "1a5721c9ce27bfd9",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "EvalResult(slicing_metrics=[((), {'': {'': {'binary_accuracy': {'doubleValue': 0.9120140953639467}, 'loss': {'doubleValue': 0.19296522438526154}, 'example_count': {'doubleValue': 9081.0}, 'auc': {'doubleValue': 0.9646401905449972}, 'precision': {'doubleValue': 0.8493771234428086}, 'recall': {'doubleValue': 0.7378258730939499}}}})], plots=[((), None)], attributions=[((), None)], config=None, data_location='', file_format='', model_location='')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow_model_analysis as tfma \n",
    "\n",
    "eval_result = './tfajarama-pipeline/Evaluator/evaluation/8/metrics-00000-of-00001.tfrecord'\n",
    "tfma_result = tfma.load_eval_result(eval_result)\n",
    "tfma_result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T14:26:36.143826Z",
     "start_time": "2025-01-14T14:26:35.950424Z"
    }
   },
   "id": "73deb0d96aa1c6b1",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
